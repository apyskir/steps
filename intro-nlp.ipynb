{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why?\n",
    "\n",
    "* Sklearn Pipelines are awesome... \n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier()),\n",
    "])\n",
    "\n",
    "...\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "grid_search.fit(data.data, data.target)\n",
    "\n",
    "\n",
    "```\n",
    "http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo, HTML\n",
    "YouTubeVideo(\"URdnFlZnlaE\", width=600,height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ... but sometimes not enough\n",
    " * wrapping keras/pytorch models in transformers is tricky\n",
    " * caching/saving intermediate outputs is not easy\n",
    " * it has to be X,y input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Airflow does it all but is just to much\n",
    "\n",
    "<img src=\"https://airflow.apache.org/_images/airflow.gif\" style=\"width: 800px;\"/>\n",
    "\n",
    "\n",
    "https://airflow.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why not build one?\n",
    "\n",
    "<img src=\"http://bonkersworld.net/img/2011.11.15_building_software.png\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "* Almost like sklearn transformers\n",
    "* Every transformer has `fit_transform` and `transform`\n",
    "\n",
    "```python\n",
    "def fit_transform(self, X, y):\n",
    "    return\n",
    "\n",
    "def transform(self, X):\n",
    "    return\n",
    "```\n",
    "\n",
    "* Those methods return `dict`\n",
    "* Inputs can be named **however** you like and can be **whatever** you like\n",
    "* Every transformer implements `save` and `load` methods\n",
    "\n",
    "```python\n",
    "from keras.models import load_model\n",
    "\n",
    "def save(self, filepath):\n",
    "    self.model.save(filepath)\n",
    "\n",
    "def load(self, filepath):\n",
    "    self.model = load_model(filepath)\n",
    "    return self\n",
    "```\n",
    "\n",
    "* They can do much **more than** just **transform data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from steps.preprocessing import TextCounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_counter = TextCounter()\n",
    "\n",
    "outputs = text_counter.fit_transform(['calculate featueres for this text',\n",
    "                                       'Get Some Features For This As Well !!!'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, output in outputs.items():\n",
    "    display(key)\n",
    "    display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load steps/keras/models.py\n",
    "import shutil\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "from ..base import BaseTransformer\n",
    "from .contrib import AttentionWeightedAverage\n",
    "from .architectures import vdcnn, scnn, dpcnn, cudnn_gru, cudnn_lstm\n",
    "\n",
    "\n",
    "class KerasModelTransformer(BaseTransformer):\n",
    "    \"\"\"\n",
    "    Todo:\n",
    "        load the best model at the end of the fit and save it\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, architecture_config, training_config, callbacks_config):\n",
    "        self.architecture_config = architecture_config\n",
    "        self.training_config = training_config\n",
    "        self.callbacks_config = callbacks_config\n",
    "\n",
    "    def reset(self):\n",
    "        self.model = self._build_model(**self.architecture_config)\n",
    "\n",
    "    def _compile_model(self, model_params, optimizer_params):\n",
    "        model = self._build_model(**model_params)\n",
    "        optimizer = self._build_optimizer(**optimizer_params)\n",
    "        loss = self._build_loss()\n",
    "        model.compile(optimizer=optimizer, loss=loss)\n",
    "        return model\n",
    "\n",
    "    def _create_callbacks(self, **kwargs):\n",
    "        return NotImplementedError\n",
    "\n",
    "    def _build_model(self, **kwargs):\n",
    "        return NotImplementedError\n",
    "\n",
    "    def _build_optimizer(self, **kwargs):\n",
    "        return NotImplementedError\n",
    "\n",
    "    def _build_loss(self, **kwargs):\n",
    "        return NotImplementedError\n",
    "\n",
    "    def save(self, filepath):\n",
    "        checkpoint_callback = self.callbacks_config.get('model_checkpoint')\n",
    "        if checkpoint_callback:\n",
    "            checkpoint_filepath = checkpoint_callback['filepath']\n",
    "            shutil.copyfile(checkpoint_filepath, filepath)\n",
    "        else:\n",
    "            self.model.save(filepath)\n",
    "\n",
    "    def load(self, filepath):\n",
    "        self.model = load_model(filepath,\n",
    "                                custom_objects={'AttentionWeightedAverage': AttentionWeightedAverage})\n",
    "        return self\n",
    "\n",
    "\n",
    "class ClassifierXY(KerasModelTransformer):\n",
    "    def fit(self, X, y, validation_data, *args, **kwargs):\n",
    "        self.callbacks = self._create_callbacks(**self.callbacks_config)\n",
    "        self.model = self._compile_model(**self.architecture_config)\n",
    "\n",
    "        self.model.fit(X, y,\n",
    "                       validation_data=validation_data,\n",
    "                       callbacks=self.callbacks,\n",
    "                       verbose=1,\n",
    "                       **self.training_config)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, validation_data=None, *args, **kwargs):\n",
    "        predictions = self.model.predict(X, verbose=1)\n",
    "        return {'prediction_probability': predictions}\n",
    "\n",
    "\n",
    "class ClassifierGenerator(KerasModelTransformer):\n",
    "    def fit(self, datagen, validation_datagen, *args, **kwargs):\n",
    "        self.callbacks = self._create_callbacks(**self.callbacks_config)\n",
    "        self.model = self._compile_model(**self.architecture_config)\n",
    "\n",
    "        train_flow, train_steps = datagen\n",
    "        valid_flow, valid_steps = validation_datagen\n",
    "        self.model.fit_generator(train_flow,\n",
    "                                 steps_per_epoch=train_steps,\n",
    "                                 validation_data=valid_flow,\n",
    "                                 validation_steps=valid_steps,\n",
    "                                 callbacks=self.callbacks,\n",
    "                                 verbose=1,\n",
    "                                 **self.training_config)\n",
    "        return self\n",
    "\n",
    "    def transform(self, datagen, validation_datagen=None, *args, **kwargs):\n",
    "        test_flow, test_steps = datagen\n",
    "        predictions = self.model.predict_generator(test_flow, test_steps, verbose=1)\n",
    "        return {'prediction_probability': predictions}\n",
    "\n",
    "\n",
    "class PretrainedEmbeddingModel(ClassifierXY):\n",
    "    def fit(self, X, y, validation_data, embedding_matrix):\n",
    "        X_valid, y_valid = validation_data\n",
    "        self.callbacks = self._create_callbacks(**self.callbacks_config)\n",
    "        self.architecture_config['model_params']['embedding_matrix'] = embedding_matrix\n",
    "        self.model = self._compile_model(**self.architecture_config)\n",
    "        self.model.fit(X, y,\n",
    "                       validation_data=[X_valid, y_valid],\n",
    "                       callbacks=self.callbacks,\n",
    "                       verbose=1,\n",
    "                       **self.training_config)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, validation_data=None, embedding_matrix=None):\n",
    "        predictions = self.model.predict(X, verbose=1)\n",
    "        return {'prediction_probability': predictions}\n",
    "\n",
    "\n",
    "class CharVDCNNTransformer(ClassifierXY):\n",
    "    def _build_model(self, embedding_size, maxlen, max_features,\n",
    "                     filter_nr, kernel_size, repeat_block,\n",
    "                     dense_size, repeat_dense, output_size, output_activation,\n",
    "                     max_pooling, mean_pooling, weighted_average_attention, concat_mode,\n",
    "                     dropout_embedding, conv_dropout, dense_dropout, dropout_mode,\n",
    "                     conv_kernel_reg_l2, conv_bias_reg_l2,\n",
    "                     dense_kernel_reg_l2, dense_bias_reg_l2,\n",
    "                     use_prelu, use_batch_norm, batch_norm_first):\n",
    "        return vdcnn(embedding_size, maxlen, max_features,\n",
    "                     filter_nr, kernel_size, repeat_block,\n",
    "                     dense_size, repeat_dense, output_size, output_activation,\n",
    "                     max_pooling, mean_pooling, weighted_average_attention, concat_mode,\n",
    "                     dropout_embedding, conv_dropout, dense_dropout, dropout_mode,\n",
    "                     conv_kernel_reg_l2, conv_bias_reg_l2,\n",
    "                     dense_kernel_reg_l2, dense_bias_reg_l2,\n",
    "                     use_prelu, use_batch_norm, batch_norm_first)\n",
    "\n",
    "\n",
    "class WordSCNNTransformer(PretrainedEmbeddingModel):\n",
    "    def _build_model(self, embedding_matrix, embedding_size, trainable_embedding, maxlen, max_features,\n",
    "                     filter_nr, kernel_size, repeat_block,\n",
    "                     dense_size, repeat_dense, output_size, output_activation,\n",
    "                     max_pooling, mean_pooling, weighted_average_attention, concat_mode,\n",
    "                     dropout_embedding, conv_dropout, dense_dropout, dropout_mode,\n",
    "                     conv_kernel_reg_l2, conv_bias_reg_l2,\n",
    "                     dense_kernel_reg_l2, dense_bias_reg_l2,\n",
    "                     use_prelu, use_batch_norm, batch_norm_first):\n",
    "        return scnn(embedding_matrix, embedding_size, trainable_embedding, maxlen, max_features,\n",
    "                    filter_nr, kernel_size, repeat_block,\n",
    "                    dense_size, repeat_dense, output_size, output_activation,\n",
    "                    max_pooling, mean_pooling, weighted_average_attention, concat_mode,\n",
    "                    dropout_embedding, conv_dropout, dense_dropout, dropout_mode,\n",
    "                    conv_kernel_reg_l2, conv_bias_reg_l2,\n",
    "                    dense_kernel_reg_l2, dense_bias_reg_l2,\n",
    "                    use_prelu, use_batch_norm, batch_norm_first)\n",
    "\n",
    "\n",
    "class WordDPCNNTransformer(PretrainedEmbeddingModel):\n",
    "    def _build_model(self, embedding_matrix, embedding_size, trainable_embedding, maxlen, max_features,\n",
    "                     filter_nr, kernel_size, repeat_block,\n",
    "                     dense_size, repeat_dense, output_size, output_activation,\n",
    "                     max_pooling, mean_pooling, weighted_average_attention, concat_mode,\n",
    "                     dropout_embedding, conv_dropout, dense_dropout, dropout_mode,\n",
    "                     conv_kernel_reg_l2, conv_bias_reg_l2,\n",
    "                     dense_kernel_reg_l2, dense_bias_reg_l2,\n",
    "                     use_prelu, use_batch_norm, batch_norm_first):\n",
    "        \"\"\"\n",
    "        Implementation of http://ai.tencent.com/ailab/media/publications/ACL3-Brady.pdf\n",
    "        \"\"\"\n",
    "        return dpcnn(embedding_matrix, embedding_size, trainable_embedding, maxlen, max_features,\n",
    "                     filter_nr, kernel_size, repeat_block,\n",
    "                     dense_size, repeat_dense, output_size, output_activation,\n",
    "                     max_pooling, mean_pooling, weighted_average_attention, concat_mode,\n",
    "                     dropout_embedding, conv_dropout, dense_dropout, dropout_mode,\n",
    "                     conv_kernel_reg_l2, conv_bias_reg_l2,\n",
    "                     dense_kernel_reg_l2, dense_bias_reg_l2,\n",
    "                     use_prelu, use_batch_norm, batch_norm_first)\n",
    "\n",
    "\n",
    "class WordCuDNNLSTMTransformer(PretrainedEmbeddingModel):\n",
    "    def _build_model(self, embedding_matrix, embedding_size, trainable_embedding,\n",
    "                     maxlen, max_features,\n",
    "                     unit_nr, repeat_block,\n",
    "                     dense_size, repeat_dense, output_size, output_activation,\n",
    "                     max_pooling, mean_pooling, weighted_average_attention, concat_mode,\n",
    "                     dropout_embedding, rnn_dropout, dense_dropout, dropout_mode,\n",
    "                     rnn_kernel_reg_l2, rnn_recurrent_reg_l2, rnn_bias_reg_l2,\n",
    "                     dense_kernel_reg_l2, dense_bias_reg_l2,\n",
    "                     use_prelu, use_batch_norm, batch_norm_first):\n",
    "        return cudnn_lstm(embedding_matrix, embedding_size, trainable_embedding,\n",
    "                          maxlen, max_features,\n",
    "                          unit_nr, repeat_block,\n",
    "                          dense_size, repeat_dense, output_size, output_activation,\n",
    "                          max_pooling, mean_pooling, weighted_average_attention, concat_mode,\n",
    "                          dropout_embedding, rnn_dropout, dense_dropout, dropout_mode,\n",
    "                          rnn_kernel_reg_l2, rnn_recurrent_reg_l2, rnn_bias_reg_l2,\n",
    "                          dense_kernel_reg_l2, dense_bias_reg_l2,\n",
    "                          use_prelu, use_batch_norm, batch_norm_first)\n",
    "\n",
    "\n",
    "class WordCuDNNGRUTransformer(PretrainedEmbeddingModel):\n",
    "    def _build_model(self, embedding_matrix, embedding_size, trainable_embedding,\n",
    "                     maxlen, max_features,\n",
    "                     unit_nr, repeat_block,\n",
    "                     dense_size, repeat_dense, output_size, output_activation,\n",
    "                     max_pooling, mean_pooling, weighted_average_attention, concat_mode,\n",
    "                     dropout_embedding, rnn_dropout, dense_dropout, dropout_mode,\n",
    "                     rnn_kernel_reg_l2, rnn_recurrent_reg_l2, rnn_bias_reg_l2,\n",
    "                     dense_kernel_reg_l2, dense_bias_reg_l2,\n",
    "                     use_prelu, use_batch_norm, batch_norm_first):\n",
    "        return cudnn_gru(embedding_matrix, embedding_size, trainable_embedding,\n",
    "                         maxlen, max_features,\n",
    "                         unit_nr, repeat_block,\n",
    "                         dense_size, repeat_dense, output_size, output_activation,\n",
    "                         max_pooling, mean_pooling, weighted_average_attention, concat_mode,\n",
    "                         dropout_embedding, rnn_dropout, dense_dropout, dropout_mode,\n",
    "                         rnn_kernel_reg_l2, rnn_recurrent_reg_l2, rnn_bias_reg_l2,\n",
    "                         dense_kernel_reg_l2, dense_bias_reg_l2,\n",
    "                         use_prelu, use_batch_norm, batch_norm_first)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step\n",
    "\n",
    "```python\n",
    "glove_dpcnn = Step(name='glove_dpcnn',\n",
    "                   transformer=WordDPCNN(**config.dpcnn_network),\n",
    "                   input_data = [],\n",
    "                   input_steps=[word_tokenizer, \n",
    "                                preprocessed_input,      \n",
    "                                glove_embeddings],\n",
    "                   adapter={'X': ([('word_tokenizer', 'X')]),\n",
    "                            'y': ([('cleaning_output', 'y')]),\n",
    "                            'embedding_matrix': ([('glove_embeddings', 'embeddings_matrix')]),\n",
    "                            'validation_data': (\n",
    "                                [('word_tokenizer', 'X_valid'), ('cleaning_output', 'y_valid')],\n",
    "                                to_tuple_inputs),\n",
    "                            },\n",
    "                   cache_dirpath=config.env.cache_dirpath,\n",
    "                   cache_output = True,\n",
    "                   save_output=False, \n",
    "                   load_saved_output=False,\n",
    "                   force_fitting=True\n",
    "                  )\n",
    "```\n",
    "\n",
    "* Building block of pipelines\n",
    "* Wraps around transformer and adds functionality\n",
    "* easy to plug in outputs from other steps and data sources with `input_steps`, `input_data` and `adapter`\n",
    "* transformers are cached/persisted as the pipeline trains (not only after it has trained)\n",
    "* outputs are cached by default but you can save outputs for debugging/inspection with `save_output`\n",
    "* if you want to always fit step even if it was fit before use `force_fitting`\n",
    "* objects are stored in the `cache_dirpath` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /mnt/ml-team/dsb_2018/kuba/trained_pipelines/weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "DAG of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from steps.preprocessing import XYSplit, TextCleaner\n",
    "from steps.keras.loaders import Tokenizer\n",
    "from steps.keras.embeddings import GloveEmbeddingsMatrix\n",
    "from steps.keras.models import WordDPCNNTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = '/mnt/ml-team/minerva/debug/ml_seminar'\n",
    "\n",
    "xy_train = Step(name='xy_train',\n",
    "            transformer=XYSplit(x_columns=['comment_text'],\n",
    "                                y_columns=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "                               ),\n",
    "            input_data=['input'],\n",
    "            adapter={'meta': ([('input', 'meta')]),\n",
    "                     'train_mode': ([('input', 'train_mode')])\n",
    "                     },\n",
    "            cache_dirpath=CACHE_DIR)\n",
    "\n",
    "text_cleaner = Step(name='text_cleaner_train',\n",
    "                transformer=TextCleaner(drop_punctuation=True,\n",
    "                                        drop_newline=True,\n",
    "                                        drop_multispaces=True,\n",
    "                                        all_lower_case=True,\n",
    "                                        fill_na_with='',\n",
    "                                        deduplication_threshold=10,\n",
    "                                        anonymize=False,\n",
    "                                        apostrophes=False,\n",
    "                                        use_stopwords=True),\n",
    "                input_steps=[xy_train],\n",
    "                adapter={'X': ([('xy_train', 'X')])},\n",
    "                cache_dirpath=CACHE_DIR)\n",
    "\n",
    "word_tokenizer = Step(name='word_tokenizer',\n",
    "                      transformer=Tokenizer(char_level=False,\n",
    "                                            maxlen=200,\n",
    "                                            num_words=10000),\n",
    "                      input_steps=[text_cleaner],\n",
    "                      adapter={'X': ([(text_cleaner.name, 'X')]),\n",
    "                               'train_mode': ([('cleaning_output', 'train_mode')])\n",
    "                               },\n",
    "                      cache_dirpath=CACHE_DIR)\n",
    "\n",
    "glove_embeddings = Step(name='glove_embeddings',\n",
    "                        transformer=GloveEmbeddingsMatrix(pretrained_filepath='glove.840B.300d.txt',\n",
    "                                                          max_features=10000,\n",
    "                                                          embedding_size=300),\n",
    "                        input_steps=[word_tokenizer],\n",
    "                        adapter={'tokenizer': ([(word_tokenizer.name, 'tokenizer')]),\n",
    "                                 },\n",
    "                        cache_dirpath=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dpcnn_config = {\n",
    "    'architecture_config': {'model_params': {'max_features': 300,\n",
    "                                             'maxlen': 200,\n",
    "                                             'embedding_size': 300,\n",
    "                                             'trainable_embedding': True,\n",
    "                                             'filter_nr': 64,\n",
    "                                             'kernel_size': 3,\n",
    "                                             'repeat_block': 6,\n",
    "                                             'dense_size': 256,\n",
    "                                             'repeat_dense': 2,\n",
    "                                             'output_size': 6,\n",
    "                                             'output_activation': 'sigmoid',\n",
    "                                             'max_pooling': True,\n",
    "                                             'mean_pooling': True,\n",
    "                                             'weighted_average_attention': False,\n",
    "                                             'concat_mode': 'concat',\n",
    "                                             'dropout_embedding': 0.5,\n",
    "                                             'conv_dropout': 0.25,\n",
    "                                             'dense_dropout': 0.25,\n",
    "                                             'dropout_mode': 'spatial',\n",
    "                                             'conv_kernel_reg_l2': 0.0,\n",
    "                                             'conv_bias_reg_l2': 0.0,\n",
    "                                             'dense_kernel_reg_l2': 0.0,\n",
    "                                             'dense_bias_reg_l2': 0.0,\n",
    "                                             'use_prelu': True,\n",
    "                                             'use_batch_norm': True,\n",
    "                                             'batch_norm_first': True,\n",
    "                                             },\n",
    "                            'optimizer_params': {'lr': 0.01,\n",
    "                                                 'momentum': 0.9,\n",
    "                                                 'nesterov': True\n",
    "                                                 },\n",
    "                            },\n",
    "    'training_config': {'epochs': 10,\n",
    "                        'shuffle': True,\n",
    "                        'batch_size': 128,\n",
    "                        },\n",
    "    'callbacks_config': {'model_checkpoint': {\n",
    "        'filepath': os.path.join(CACHE_DIR, 'checkpoints', 'dpcnn_network', 'best_model.h5'),\n",
    "        'save_best_only': True,\n",
    "        'save_weights_only': False},\n",
    "        'lr_scheduler': {'gamma': 0.95},\n",
    "        'unfreeze_layers': {'unfreeze_on_epoch': 10},\n",
    "        'early_stopping': {'patience': 5},\n",
    "        'neptune_monitor': {'model_name': 'dpcnn'},\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dpcnn = Step(name='glove_dpcnn',\n",
    "                   transformer=WordDPCNNTransformer(**dpcnn_config),\n",
    "                   input_steps=[word_tokenizer, xy_train, glove_embeddings],\n",
    "                   adapter={'X': ([('word_tokenizer', 'X')]),\n",
    "                            'y': ([('xy_train', 'y')]),\n",
    "                            'embedding_matrix': ([('glove_embeddings', 'embeddings_matrix')]),\n",
    "                            },\n",
    "                   cache_dirpath=CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dpcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_step = glove_dpcnn.get_step('word_tokenizer')\n",
    "intermediate_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_step.transformer.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Inference\n",
    "\n",
    "Just run `fit_transform` on last the very last step and all steps will be fitted recursively\n",
    "```python\n",
    "    data_train = {'input': {'meta': train,\n",
    "                            'meta_valid': valid,\n",
    "                            'train_mode': True,\n",
    "                      },\n",
    "            }\n",
    "    train_predictions = glove_dpcnn.fit_transform(data_train)\n",
    "```\n",
    "\n",
    "prediction will be done on `transform`\n",
    "\n",
    "```python\n",
    "    data_inference = {'input': {'meta': test,\n",
    "                                'meta_valid': None,\n",
    "                                'train_mode': False,\n",
    "                      },\n",
    "            }\n",
    "    train_predictions = glove_dpcnn.transform(data_inference)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is missing?\n",
    "* automatic sklean/steps conversion\n",
    "\n",
    "```python\n",
    "\n",
    "from steps.base import make_step\n",
    "\n",
    "step_transformer = make_step(SklearnTransformer())\n",
    "step_transformer = make_step(Pipeline())\n",
    "```\n",
    "\n",
    "* automatic grid search\n",
    "\n",
    "```python\n",
    "\n",
    "xgboost_ensemble = Step(name='xgboost_ensemble',\n",
    "                        transformer=XGBoostClassifierMultilabel(**config.xgboost_ensemble),\n",
    "                        input_data=['input'],\n",
    "                        cache_dirpath=CACHE_DIR,\n",
    "                        grid_search_params=parameter_space,\n",
    "                        grid_runs=100,\n",
    "                        grid_search_method='hyperopt')\n",
    "```\n",
    "\n",
    "* paralelization\n",
    "* automatic multistep bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's talk toxic\n",
    "https://github.com/neptune-ml/kaggle-toxic-starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_py3",
   "language": "python",
   "name": "dl_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
